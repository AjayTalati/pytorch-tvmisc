{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "016D036D46DD4C408D48EFA828B4E77C"
   },
   "source": [
    "# Batch Sinkhorn Iteration Wasserstein Distance\n",
    "\n",
    "Thomas Viehmann\n",
    "\n",
    "This notebook implements sinkhorn iteration wasserstein distance layers.\n",
    "\n",
    "## Important note: This is under construction and does not yet work as well as it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "4FA01C447B594B37B4828D7A42DE20EB",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable, Function\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "604CDE3AD9E94ECD8EAF9F423C06744D"
   },
   "source": [
    "The following is a \"plain sinkhorn\" implementation that could be used in\n",
    "[C. Frogner et. al.: Learning with a Wasserstein Loss](https://arxiv.org/abs/1506.05439)\n",
    "\n",
    "Note that we use a different convention for $\\lambda$ (i.e. we use $\\lambda$ as the weight for the regularisation, later versions of the above use $\\lambda^-1$ as the weight).\n",
    "\n",
    "The implementation has benefitted from\n",
    "\n",
    "- Chiyuan Zhang's implementation in [Mocha](https://github.com/pluskid/Mocha.jl),\n",
    "- Rémi Flamary's implementation of various sinkhorn algorithms in [Python Optimal Transport](https://github.com/rflamary/POT)\n",
    "\n",
    "Thank you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "cell_id": "E3421C815A6B4EB1B47A95F053B78BFF",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WassersteinLossVanilla(Function):\n",
    "    def __init__(self,cost, lam = 1e-3, sinkhorn_iter = 50):\n",
    "        super(WassersteinLossVanilla,self).__init__()\n",
    "        \n",
    "        # cost = matrix M = distance matrix\n",
    "        # lam = lambda of type float > 0\n",
    "        # sinkhorn_iter > 0\n",
    "        # diagonal cost should be 0\n",
    "        self.cost = cost\n",
    "        self.lam = lam\n",
    "        self.sinkhorn_iter = sinkhorn_iter\n",
    "        self.na = cost.size(0)\n",
    "        self.nb = cost.size(1)\n",
    "        self.K = torch.exp(-self.cost/self.lam)\n",
    "        self.KM = self.cost*self.K\n",
    "        self.stored_grad = None\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"pred: Batch * K: K = # mass points\n",
    "           target: Batch * L: L = # mass points\"\"\"\n",
    "        assert pred.size(1)==self.na\n",
    "        assert target.size(1)==self.nb\n",
    "\n",
    "        nbatch = pred.size(0)\n",
    "        \n",
    "        u = self.cost.new(nbatch, self.na).fill_(1.0/self.na)\n",
    "        \n",
    "        for i in range(self.sinkhorn_iter):\n",
    "            v = target/(torch.mm(u,self.K.t())) # double check K vs. K.t() here and next line\n",
    "            u = pred/(torch.mm(v,self.K))\n",
    "            #print (\"stability at it\",i, \"u\",(u!=u).sum(),u.max(),\"v\", (v!=v).sum(), v.max())\n",
    "            if (u!=u).sum()>0 or (v!=v).sum()>0 or u.max()>1e9 or v.max()>1e9: # u!=u is a test for NaN...\n",
    "                # we have reached the machine precision\n",
    "                # come back to previous solution and quit loop\n",
    "                raise Exception(str(('Warning: numerical errrors',i+1,\"u\",(u!=u).sum(),u.max(),\"v\",(v!=v).sum(),v.max())))\n",
    "\n",
    "        loss = (u*torch.mm(v,self.KM.t())).mean(0).sum() # double check KM vs KM.t()...\n",
    "        grad = self.lam*u.log()/nbatch # check whether u needs to be transformed        \n",
    "        grad = grad-torch.mean(grad,dim=1).expand_as(grad)\n",
    "        grad = grad-torch.mean(grad,dim=1).expand_as(grad) # does this help over only once?\n",
    "        self.stored_grad = grad\n",
    "\n",
    "        dist = self.cost.new((loss,))\n",
    "        return dist\n",
    "    def backward(self, grad_output):\n",
    "        #print (grad_output.size(), self.stored_grad.size())\n",
    "        return self.stored_grad*grad_output[0],None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "33E33D188E5D4F5A8E61D23599F01A6A"
   },
   "source": [
    "The following is a variant of the \"log-stabilized sinkhorn\" algorithm as described by [B. Schmitzer: Stabilized Sparse Scaling Algorithms for Entropy Regularized Transport Problems](https://arxiv.org/abs/1610.06519).\n",
    "However, the author (for his application of computing the transport map for a single pair of measures) uses a form that modifies the $K$ matrix. This makes is less suitable for processing (mini-) batches, where we want to avoid the additional dimension.\n",
    "\n",
    "To the best of my knowledge, this is the first implementation of a batch stabilized sinkhorn algorithm and I would appreciate if you find it useful, you could credit\n",
    "*Thomas Viehmann: Batch Sinkhorn Iteration Wasserstein Distance*, [https://github.com/t-vi/pytorch-tvmisc/wasserstein-distance/Pytorch_Wasserstein.ipynb](https://github.com/t-vi/pytorch-tvmisc/wasserstein-distance/Pytorch_Wasserstein.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "cell_id": "C12F43EA5C8F4519841D74209DDF5F0B",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WassersteinLossStab(Function):\n",
    "    def __init__(self,cost, lam = 1e-3, sinkhorn_iter = 50):\n",
    "        super(WassersteinLossStab,self).__init__()\n",
    "        \n",
    "        # cost = matrix M = distance matrix\n",
    "        # lam = lambda of type float > 0\n",
    "        # sinkhorn_iter > 0\n",
    "        # diagonal cost should be 0\n",
    "        self.cost = cost\n",
    "        self.lam = lam\n",
    "        self.sinkhorn_iter = sinkhorn_iter\n",
    "        self.na = cost.size(0)\n",
    "        self.nb = cost.size(1)\n",
    "        self.K = torch.exp(-self.cost/self.lam)\n",
    "        self.KM = self.cost*self.K\n",
    "        self.stored_grad = None\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"pred: Batch * K: K = # mass points\n",
    "           target: Batch * L: L = # mass points\"\"\"\n",
    "        assert pred.size(1)==self.na\n",
    "        assert target.size(1)==self.nb\n",
    "\n",
    "        batch_size = pred.size(0)\n",
    "        \n",
    "        log_a, log_b = torch.log(pred), torch.log(target)\n",
    "        log_u = self.cost.new(batch_size, self.na).fill_(-numpy.log(self.na))\n",
    "        log_v = self.cost.new(batch_size, self.nb).fill_(-numpy.log(self.nb))\n",
    "        \n",
    "        for i in range(self.sinkhorn_iter):\n",
    "            log_u_max = torch.max(log_u, dim=1)[0]\n",
    "            u_stab = torch.exp(log_u-log_u_max.expand_as(log_u))\n",
    "            log_v = log_b - torch.log(torch.mm(self.K.t(),u_stab.t()).t()) - log_u_max.expand_as(log_v)\n",
    "            log_v_max = torch.max(log_v, dim=1)[0]\n",
    "            v_stab = torch.exp(log_v-log_v_max.expand_as(log_v))\n",
    "            log_u = log_a - torch.log(torch.mm(self.K, v_stab.t()).t()) - log_v_max.expand_as(log_u)\n",
    "\n",
    "        log_v_max = torch.max(log_v, dim=1)[0]\n",
    "        v_stab = torch.exp(log_v-log_v_max.expand_as(log_v))\n",
    "        logcostpart1 = torch.log(torch.mm(self.KM,v_stab.t()).t())+log_v_max.expand_as(log_u)\n",
    "        wnorm = torch.exp(log_u+logcostpart1).mean(0).sum() # sum(1) for per item pair loss...\n",
    "        grad = log_u*self.lam\n",
    "        grad = grad-torch.mean(grad,dim=1).expand_as(grad)\n",
    "        grad = grad-torch.mean(grad,dim=1).expand_as(grad) # does this help over only once?\n",
    "        grad = grad/batch_size\n",
    "        \n",
    "        self.stored_grad = grad\n",
    "\n",
    "        return self.cost.new((wnorm,))\n",
    "    def backward(self, grad_output):\n",
    "        #print (grad_output.size(), self.stored_grad.size())\n",
    "        #print (self.stored_grad, grad_output)\n",
    "        res = grad_output.new()\n",
    "        res.resize_as_(self.stored_grad).copy_(self.stored_grad)\n",
    "        if grad_output[0] != 1:\n",
    "            res.mul_(grad_output[0])\n",
    "        return res,None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "076665B313D4466B91C846CCC18BA44B"
   },
   "source": [
    "We may test our implementation against Rémi Flamary's algorithms in [Python Optimal Transport](https://github.com/rflamary/POT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "cell_id": "0EDD7D978F64462880A2D1CFE6833739",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ot\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "cell_id": "F0772ABA3E824ADF8792A13CBAB82F1F",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test problem from Python Optimal Transport\n",
    "n=100\n",
    "a=ot.datasets.get_1D_gauss(n,m=20,s=10).astype(numpy.float32)\n",
    "b=ot.datasets.get_1D_gauss(n,m=60,s=30).astype(numpy.float32)\n",
    "c=ot.datasets.get_1D_gauss(n,m=40,s=20).astype(numpy.float32)\n",
    "a64=ot.datasets.get_1D_gauss(n,m=20,s=10).astype(numpy.float64)\n",
    "b64=ot.datasets.get_1D_gauss(n,m=60,s=30).astype(numpy.float64)\n",
    "c64=ot.datasets.get_1D_gauss(n,m=40,s=20).astype(numpy.float64)\n",
    "# distance function\n",
    "x=numpy.arange(n,dtype=numpy.float32)\n",
    "M=(x[:,numpy.newaxis]-x[numpy.newaxis,:])**2\n",
    "M/=M.max()\n",
    "x64=numpy.arange(n,dtype=numpy.float64)\n",
    "M64=(x64[:,numpy.newaxis]-x64[numpy.newaxis,:])**2\n",
    "M64/=M64.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "cell_id": "33772D05E8344EA590D3AA22F3B0709C",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transp = ot.bregman.sinkhorn(a,b,M,reg=1e-3)\n",
    "transp2 = ot.bregman.sinkhorn_stabilized(a,b,M,reg=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "cell_id": "D1952592EE634C83BE396862915C82F0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15025606400382638, 0.1502669613228855)"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(transp*M).sum(), (transp2*M).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "cell_id": "C1078B34587145C981BD4818A57AE40D",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cabt = Variable(torch.from_numpy(numpy.stack((c,a,b),axis=0)))\n",
    "abct = Variable(torch.from_numpy(numpy.stack((a,b,c),axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "cell_id": "070CF22C32154E778C882A19143A22B3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  0.1053\n",
       " [torch.FloatTensor of size 1], (Variable containing:\n",
       "   0.1053\n",
       "  [torch.FloatTensor of size 1], Variable containing:\n",
       "  1.00000e-02 *\n",
       "    7.5846\n",
       "  [torch.FloatTensor of size 1], Variable containing:\n",
       "   0.1773\n",
       "  [torch.FloatTensor of size 1], Variable containing:\n",
       "  1.00000e-02 *\n",
       "    6.2796\n",
       "  [torch.FloatTensor of size 1]))"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossvanilla = WassersteinLossVanilla(torch.from_numpy(M), lam=0.1)\n",
    "loss = lossvanilla\n",
    "losses = loss(cabt,abct), loss(cabt[:1],abct[:1]), loss(cabt[1:2],abct[1:2]), loss(cabt[2:],abct[2:])\n",
    "sum(losses[1:])/3, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "cell_id": "5AF1937602A240BF95BCDFCC730175AA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  0.1053\n",
       " [torch.FloatTensor of size 1], (Variable containing:\n",
       "   0.1053\n",
       "  [torch.FloatTensor of size 1], Variable containing:\n",
       "  1.00000e-02 *\n",
       "    7.5846\n",
       "  [torch.FloatTensor of size 1], Variable containing:\n",
       "   0.1773\n",
       "  [torch.FloatTensor of size 1], Variable containing:\n",
       "  1.00000e-02 *\n",
       "    6.2796\n",
       "  [torch.FloatTensor of size 1]))"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = WassersteinLossStab(torch.from_numpy(M), lam=0.1)\n",
    "losses = loss(cabt,abct), loss(cabt[:1],abct[:1]), loss(cabt[1:2],abct[1:2]), loss(cabt[2:],abct[2:])\n",
    "sum(losses[1:])/3, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "701438D79C48420AB97E17AEF6C8B76E"
   },
   "source": [
    "The stabilized version can handle the extended range needed to get closer to the Python Optimal Transport loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "cell_id": "DC144D4A21074F0B9CF68E1006A8A781"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15445974773824594, 0.15445145964622498)"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transp3 = ot.bregman.sinkhorn_stabilized(a,b,M,reg=1e-2)\n",
    "loss = WassersteinLossStab(torch.from_numpy(M), lam=0.01)\n",
    "(transp3*M).sum(), loss(cabt[1:2],abct[1:2]).data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5A983A5E9B2C4A809C6AE0AE3B4704B7"
   },
   "source": [
    "By the linear expansion, we should have\n",
    "$$\n",
    "L(x2) \\approx L(x1)+\\nabla L(\\frac{x1+x2}{2})(x2-x1),\n",
    "$$\n",
    "so in particular we can see if for an example\n",
    "$L(x+\\epsilon \\nabla L)-L(x1) / \\epsilon \\|\\nabla L\\|^2 \\approx 1$.\n",
    "\n",
    "This seems to be the case ... sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "cell_id": "CDCD8C5F655248578FAE425282E913F5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.9228\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theloss = WassersteinLossStab(torch.from_numpy(M), lam=0.01, sinkhorn_iter=50)\n",
    "cabt = Variable(torch.from_numpy(numpy.stack((c,a,b),axis=0)))\n",
    "abct = Variable(torch.from_numpy(numpy.stack((a,b,c),axis=0)),requires_grad=True)\n",
    "lossv1 = theloss(abct,cabt)\n",
    "lossv1.backward()\n",
    "grv = abct.grad\n",
    "epsilon = 1e-5\n",
    "abctv2 = Variable(abct.data-epsilon*grv.data, requires_grad=True)\n",
    "lossv2 = theloss(abctv2, cabt)\n",
    "lossv2.backward()\n",
    "grv2 = abctv2.grad\n",
    "(lossv1.data-lossv2.data)/(epsilon*((0.5*(grv.data+grv2.data))**2).sum()) # should be around 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "E1E7785FCF2E4754B2992D07ADBE636A",
    "collapsed": true
   },
   "source": [
    "Naturally, one has to check whether the abctv2 is a valid probability distribution (i.e. all entries $>0$). It seems that the range of $\\lambda$ in which the gradient works well is somewhat limited. This may point to a bug in the implementation.\n",
    "\n",
    "Note also that feeding the same distribution in both arguments results in a NaN, when 0 is the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "E7C8C65A1F454D828D7CCEEE431EFFFC",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
